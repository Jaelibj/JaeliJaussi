[
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html",
    "href": "Full_Stack/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-1-import-pandas-and-load-the-data",
    "href": "notebooks/Exploration_02.html#part-1-import-pandas-and-load-the-data",
    "title": "Data Exploration 02",
    "section": "Part 1: Import Pandas and load the data",
    "text": "Part 1: Import Pandas and load the data\nRemember to import Pandas the conventional way. If you‚Äôve forgotten how, you may want to review Data Exploration 01.\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce you‚Äôve loaded the data, it‚Äôs a good idea to poke around a little bit to find out what you‚Äôre dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 1: Enter your code below to import Pandas according to the\n# conventional method. Then load the dataset into a Pandas dataframe.\nimport pandas as pd\n\ncereal = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\")\n\n# Write any code needed to explore the data by seeing what the first few\n# rows look like. Then display a technical summary of the data to determine\n# the data types of each column, and which columns have missing data.\ncereal.head()\n\n\n    \n\n\n\n\n\n\nname\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-2-calculate-summary-statistics",
    "href": "notebooks/Exploration_02.html#part-2-calculate-summary-statistics",
    "title": "Data Exploration 02",
    "section": "Part 2: Calculate Summary Statistics",
    "text": "Part 2: Calculate Summary Statistics\nThe marketing team has determined that when choosing a cereal, consumers are most interested in calories, sugars, fiber, fat, and protein.\nFirst, let‚Äôs calcuate some summary statistics for these categories across the entire dataset. We‚Äôre particularly intrested in the mean, median, standard deviation, min, and max values.\nThere are multiple ways to accomplish this.\n\n# Part 2: Enter your code below to calculate summary statistics for the\n# calories, sugars, fiber, fat, and protein features.\n\ncereal[['calories', 'sugars', 'fiber', 'fat', 'protein']].describe()\n\n\n    \n\n\n\n\n\n\ncalories\nsugars\nfiber\nfat\nprotein\n\n\n\n\ncount\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n\n\nmean\n106.883117\n6.922078\n2.151948\n1.012987\n2.545455\n\n\nstd\n19.484119\n4.444885\n2.383364\n1.006473\n1.094790\n\n\nmin\n50.000000\n-1.000000\n0.000000\n0.000000\n1.000000\n\n\n25%\n100.000000\n3.000000\n1.000000\n0.000000\n2.000000\n\n\n50%\n110.000000\n7.000000\n2.000000\n1.000000\n3.000000\n\n\n75%\n110.000000\n11.000000\n3.000000\n2.000000\n3.000000\n\n\nmax\n160.000000\n15.000000\n14.000000\n5.000000\n6.000000"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-3-transform-data",
    "href": "notebooks/Exploration_02.html#part-3-transform-data",
    "title": "Data Exploration 02",
    "section": "Part 3: Transform Data",
    "text": "Part 3: Transform Data\nTo make analysis easier, you want to convert the manufacturer codes used in the dataset to the manufacturer names.\nFirst, display the count of each manufacturer code value used in the dataset (found in the mfr column).\nThen, create a new column with the appropriate manufacturer name for each entry, using this mapping:\nA = American Home Food Products\nG = General Mills\nK = Kelloggs\nN = Nabisco\nP = Post\nQ = Quaker Oats\nR = Ralston Purina\n\nNote: While the tutorial linked above uses the replace function, using the map function instead can often be much faster and more memory efficient, especially for large datasets.\n\n\n# Display the count of values for the manufacturer code (\"mfr\" column), then\n# create a new column containing the appropriate manufacturer names.\ncereal['mfr_name'] = cereal['mfr'].map({'A': 'American Home Food Products',\n    'G': 'General Mills',\n    'K': 'Kelloggs',\n    'N': 'Nabisco',\n    'P': 'Post',\n    'Q': 'Quaker Oats',\n    'R': 'Ralston Purina'})"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-4-visualization",
    "href": "notebooks/Exploration_02.html#part-4-visualization",
    "title": "Data Exploration 02",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\nLet‚Äôs do some more data exploration visually.\nImport your visualization library of choice and set any needed configuration options.\n\n# Import your visualization library\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Select Color\nsns.set(style=\"whitegrid\")\n\n\nSugar Distribution\nMarketing tells us that their surveys have revealed that sugar content is the number one concern of consumers when choosing cereal.\nThey would like to see the following visualizations:\n\nA histogram plot of the sugar content in all cereals.\nA scatter plot showing the relationship between sugar and calories.\nA box plot showing the distribution of sugar content by manufacturer.\n\n\n# Create the three visualzations requested by the the marketing team\n# A histogram plot of the sugar content in all cereals.\nplt.figure(figsize=(8, 5))\nsns.histplot(data=cereal, x='sugars', bins=10)\nplt.title(\"Distribution of Sugar Content in Cereals\")\nplt.xlabel(\"Sugar (grams)\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# A scatter plot showing the relationship between sugar and calories.\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=cereal, x='sugars', y='calories')\nplt.title(\"Sugar vs Calories in Cereals\")\nplt.xlabel(\"Sugar (grams)\")\nplt.ylabel(\"Calories\")\nplt.show()\n\n# A box plot showing the distribution of sugar content by manufacturer.\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=cereal, x='mfr_name', y='sugars')\nplt.title(\"Sugar Content by Manufacturer\")\nplt.xlabel(\"Manufacturer\")\nplt.ylabel(\"Sugar (grams)\")\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "notebooks/Exploration_02.html#above-and-beyond",
    "href": "notebooks/Exploration_02.html#above-and-beyond",
    "title": "Data Exploration 02",
    "section": "üåü Above and Beyond üåü",
    "text": "üåü Above and Beyond üåü\nThe marketing team is pleased with what you‚Äôve accomplished so far. They have a meeting with top cereal executives in the morning, and they‚Äôd like you to do as many of the following additional tasks as you have time for:\n\nWeight Watchers used to have an older points system that used this formula: (calories / 50) + (fat / 12) - (fiber / 5), but only the first 4 grams of fiber were included in the calculation. For comparison‚Äôs sake, create an additional column with the calculation for the old points system.\nMarketing really likes the boxplot of the sugar content for each cereal, they‚Äôd like similar plots for calories and fat, but using different color schemes for each chart."
  },
  {
    "objectID": "notebooks/Exploration_01.html#mpaa-movie-ratings",
    "href": "notebooks/Exploration_01.html#mpaa-movie-ratings",
    "title": "Data Exploration 01",
    "section": "MPAA Movie Ratings:",
    "text": "MPAA Movie Ratings:\n\nG: All ages admitted.\nPG: Some material may not be suitable for children.\nPG-13: Some material may be inappropriate for children under 13.\nR: Under 17 requires accompanying parent or adult guardian\nNC-17: No One 17 and Under Admitted\n\nMost people would consider G and PG as ratings suitable for children. However, not everyone would agree that a PG-13 movie is necssarily a children‚Äôs movie. It is up to you to decide how to handle this."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-1-import-pandas",
    "href": "notebooks/Exploration_01.html#part-1-import-pandas",
    "title": "Data Exploration 01",
    "section": "Part 1: Import Pandas",
    "text": "Part 1: Import Pandas\nThe pandas library is a python library used for data analysis and manipulation. It will provide the core functionality for most of what you do in the data exploration and preprocessing stages of most machine learning projects.\nPlease see this Getting Started Guide for information on the conventional way to import Pandas into your project, as well as other helpful tips for common Pandas tasks.\n\n# Part 1: Enter the code below to import Pandas according to the\n# conventional method."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-2-load-the-data",
    "href": "notebooks/Exploration_01.html#part-2-load-the-data",
    "title": "Data Exploration 01",
    "section": "Part 2: Load the data",
    "text": "Part 2: Load the data\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce you‚Äôve loaded the data, it‚Äôs a good idea to poke around a little bit to find out what you‚Äôre dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 2: Load the dataset into a Pandas dataframe.\n\n\n# Then, explore the data by seeing what the first few rows look like.\n\n\n# Next, display a technical summary of the data to determine the data types of each column, and which columns have missing data."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-3-filter-the-data",
    "href": "notebooks/Exploration_01.html#part-3-filter-the-data",
    "title": "Data Exploration 01",
    "section": "Part 3: Filter the Data",
    "text": "Part 3: Filter the Data\nSince we‚Äôre just interested in movies, we‚Äôll need to filter out anything that isn‚Äôt a movie for our analysis. The type feature contains this information.\nOnce we have the subset, we should see how many rows it contains. There are a variety of ways to get the length of a data frame.\n\n# Use pandas's filtering abilitites to select the subset of data\n# that represents movies, then calculate how many rows are in the filtered data.\n\n\nMPAA Ratings\nNow that we have only movies, let‚Äôs get a quick count of the values being used in the rating feature.\n\n# Determine the number of records for each value of the \"rating\" feature.\n# Remember to count the values in your subset only, not in the original dataframe.\n\n\n\nMore Filtering\nThere are apparently some ‚Äúmade for TV‚Äù movies in the list that don‚Äôt fit the MPAA rating scheme.\nLet‚Äôs filter some more to just see movies rated with the standard MPAA ratings of G, PG, PG-13, R, and NC-17.\n\n# Filter the list of movies to select a new subset containing only movies with\n# a standard MPAA rating. Calculate how many rows are in this new set, and\n# then see which ratings appear most often."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-4-visualization",
    "href": "notebooks/Exploration_01.html#part-4-visualization",
    "title": "Data Exploration 01",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\nNow that we have explored and preprocessed our data, let‚Äôs create a visualization to summarize our findings.\n\nExploration vs Presentation\nBroadly speaking, there are two types of visualizations: * Barebones visualizations you might use to get a quick, visual understanding of the data while you‚Äôre trying to decide how it all fits together. * Presentation-quality visualizations that you would include in a report or presentation for management or other stakeholders.\n\n\nVisualization Tools\nThere are many different visualization tools availble. In the sections below, we‚Äôll explore the three most common. Each of these libraries has strengths and weaknesses.\nIt is probably a good idea for you to become familiar with each one, and then become proficient at whichever one you like the best.\n\n\nAltair\nThe Altair visualization library provides a large variety of very easy to use statistical charting tools.\nAltair uses a declarative language to build up charts piece by piece.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nalt.Chart(employees).mark_boxplot().encode(\n    x='job',\n    y='salary'\n)\n\n# Make a box plot style categorical plot, and customize the results\nalt.Chart(employees).mark_boxplot().encode(\n    alt.X('job', title='Job title'),\n    alt.Y('salary', title='Annual salary in thousands of $USD')\n).properties(\n  title='Salaries by Job Title'\n)\nLike with Pandas, there is a conventional way to import Altair into your projects.\n\n# Import the Altair library the conventional way.\n\nLet‚Äôs create a barchart showing the count of each movie rating by using Altair‚Äôs aggregation capabilities.\nIn this example, we see the x axis being set to a feature called a, and the y axis set to the average() of a feature called b.\nIn our case, we want the x axis to be set to rating and the y axis to be the count() of rating.\n\n# Use Altair to create a bar chart comparing the count of each movie rating\n\n\n\nSeaborn\nWhile Altair uses a ‚Äúdeclarative‚Äù syntax for building charts piece by piece, the Seaborn library provides a large variety of pre-made charts for common statistical needs.\nThese charts are divided into different categories. Each category has a high-level interface you can use for simplicity, and then a specific function for each chart that you can use if you need more control over how the chart looks.\nSeaborn uses matplotlib for its drawing, and the chart-specific functions each return a matplitlib axes object if you need additional customization.\nFor example, there are several different types of categorical plots in seaborn: bar plots, box plots, point plots, count plots, swarm plots, etc‚Ä¶\nEach of these plots can be accessed using the catplot function.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nsns.catplot(data=employees, x='job', y='salary', kind='box')\n\n# Make a swarm plot style categorical plot\nsns.catplot(data=employees, x='job', y='salary', kind='swarm')\nAlternatively, you can use the plot specific functions to give yourself more control over the output by using matplotlib functions:\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = sns.boxplot(data=employees, x='job', y='salary')\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\nLike with Pandas, there is a conventional way to import Seaborn into your projects.\nOptionally, you may wish to set some default chart aesthetics by setting the chart style.\n\n# Import the seaborn library the conventional way. Then optionally configure\n# the default chart style.\n\nSince the rating column uses categorical data, we need to use Seaborn‚Äôs categorical visualizations.\nIn particular, we want a ‚Äúcount plot‚Äù that will display a count of movie ratings.\n\n# Use seaborn to create a count plot comparing the count of each movie rating\n\n\n\nPandas built-in plotting\nIn addition to libraries like Altair and Seaborn, Pandas has some built in charting functionality.\nWhile not as sophisticated as some of the other options, it is often good enough for quick visualizations.\nJust like with seaborn‚Äôs plotting functions, the pandas plotting functions return matplotlib axes objects, which can be further customized.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nemployees[ ['job','salary'] ].plot.box()\n\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = employees[ ['job','salary'] ].plot().box()\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\n\n# Use pandas' built in plotting functions to create a count plot comparing the count of each movie rating\n# This will be a little trickier than the other libraries, but one hint is that the pandas value_counts() function\n# actually returns a dataframe."
  },
  {
    "objectID": "notebooks/Exploration_01.html#above-and-beyond",
    "href": "notebooks/Exploration_01.html#above-and-beyond",
    "title": "Data Exploration 01",
    "section": "üåü Above and Beyond üåü",
    "text": "üåü Above and Beyond üåü\nAfter reviewing your findings, the watchdog group would like some additional questions answered:\n\nHow are things affected if you include the ‚Äúmade for TV movies‚Äù that have been assigned TV ratings in your analysis, but still exclude unrated movies?\nThey would also like to see a separate report that includes only TV shows.\nFor an upcoming community meeting, the group would like to present a simple chart showing ‚ÄúFor Kids‚Äù and ‚ÄúFor Adults‚Äù categories. The easiest way to accomplish this would be to create a new column in your data frame that maps each rating to the appropriate ‚ÄúFor Kids‚Äù or ‚ÄúFor Adults‚Äù label, then create a new visualization based on that column."
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 1"
    ]
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 2"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "Physicist, Mathematician, Cambridge professor.\n\nisaac@applesdofall.org | My wikipedia page\n\n\n\nStanding on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples.\n\n\n\n\n1654-1660 The King‚Äôs School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow\n\n\n\n\n2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France\n\n\n\n\n\n\n1669 Newton Sir I, De analysi per √¶quationes numero terminorum infinitas.\n1669 Lectiones optic√¶.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001\n\n\n\n\n1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "Standing on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "1654-1660 The King‚Äôs School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow"
  },
  {
    "objectID": "resume.html#awards",
    "href": "resume.html#awards",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "1669 Newton Sir I, De analysi per √¶quationes numero terminorum infinitas.\n1669 Lectiones optic√¶.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Isaac Newtons‚Äôs CV",
    "section": "",
    "text": "1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html",
    "href": "Machine_Learning/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 1"
    ]
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project3.html",
    "href": "Competition/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 3"
    ]
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 4"
    ]
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "Cleansing_Exploration/project3.html",
    "href": "Cleansing_Exploration/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/starter_bikes.html",
    "href": "notebooks/starter_bikes.html",
    "title": "Jaeli Jaussi - Data Science Portfolio",
    "section": "",
    "text": "import pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nbikes = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n\nimport pandas as pd import numpy as np\nMake numpy values easier to read. np.set_printoptions(precision=3, suppress=True)\nimport tensorflow as tf from tensorflow.keras import layers\nimport numpy as np import pandas as pd import tensorflow as tf\nSHUFFLE_BUFFER = 500 BATCH_SIZE = 2\nnumeric_feature_names = [‚Äòage‚Äô, ‚Äòthalach‚Äô, ‚Äòtrestbps‚Äô, ‚Äòchol‚Äô, ‚Äòoldpeak‚Äô] numeric_features = df[numeric_feature_names] numeric_features.head()\nnormalizer = tf.keras.layers.Normalization(axis=-1) normalizer.adapt(np.array(numeric_features))\nnormalizer(numeric_features.iloc[:3])\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nWhat approach will we use to find the optimal learning rate?\n\n# Load data\ndf = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n\n\n# Convert date\ndf['dteday'] = pd.to_datetime(df['dteday'])\n\n# Create total rentals (target variable)\ndf['total'] = df['casual'] + df['registered']\n\n# Drop columns we won't use\ndf = df.drop(columns=['casual', 'registered'])\n\n# One-hot encode categorical variables\ndf = pd.get_dummies(df, columns=['season', 'weathersit'], drop_first=True)\n\n# Features and target\nX = df.drop(columns=['dteday', 'total'])\ny = df['total']\n\n# Scale features\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train/test split\nX_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n\n# Learning Model Trial\ndef build_model(lr):\n    model = Sequential([\n        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        Dense(32, activation='relu'),\n        Dense(1)\n    ])\n\n    model.compile(\n        optimizer=Adam(learning_rate=lr),\n        loss='mse',\n        metrics=['mae']\n    )\n\n    return model\n\nlearning_rates = [0.1, 0.01, 0.001, 0.0001]\nhistory_dict = {}\n\nfor lr in learning_rates:\n    print(f\"\\nTraining with learning rate: {lr}\")\n\n    model = build_model(lr)\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=20,\n        batch_size=32,\n        verbose=0\n    )\n\n    history_dict[lr] = history.history['val_loss']\n\n\n\nTraining with learning rate: 0.1\n\n\n/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\nTraining with learning rate: 0.01\n\nTraining with learning rate: 0.001\n\nTraining with learning rate: 0.0001\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\n/tmp/ipython-input-3371990179.py in &lt;cell line: 0&gt;()\n     23     model = build_model(lr)\n     24 \n---&gt; 25     history = model.fit(\n     26         X_train, y_train,\n     27         validation_data=(X_val, y_val),\n\n/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)\n    115         filtered_tb = None\n    116         try:\n--&gt; 117             return fn(*args, **kwargs)\n    118         except Exception as e:\n    119             filtered_tb = _process_traceback_frames(e.__traceback__)\n\n/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\n    375                 for step, iterator in epoch_iterator:\n    376                     callbacks.on_train_batch_begin(step)\n--&gt; 377                     logs = self.train_function(iterator)\n    378                     callbacks.on_train_batch_end(step, logs)\n    379                     if self.stop_training:\n\n/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py in function(iterator)\n    218                 iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n    219             ):\n--&gt; 220                 opt_outputs = multi_step_on_iterator(iterator)\n    221                 if not opt_outputs.has_value():\n    222                     raise StopIteration\n\n/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)\n    148     filtered_tb = None\n    149     try:\n--&gt; 150       return fn(*args, **kwargs)\n    151     except Exception as e:\n    152       filtered_tb = _process_traceback_frames(e.__traceback__)\n\n/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py in __call__(self, *args, **kwds)\n    831 \n    832       with OptionalXlaContext(self._jit_compile):\n--&gt; 833         result = self._call(*args, **kwds)\n    834 \n    835       new_tracing_count = self.experimental_get_tracing_count()\n\n/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py in _call(self, *args, **kwds)\n    876       # In this case we have not created variables on the first call. So we can\n    877       # run the first trace but we should fail if variables are created.\n--&gt; 878       results = tracing_compilation.call_function(\n    879           args, kwds, self._variable_creation_config\n    880       )\n\n/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py in call_function(args, kwargs, tracing_options)\n    130   args = args if args else ()\n    131   kwargs = kwargs if kwargs else {}\n--&gt; 132   function = trace_function(\n    133       args=args, kwargs=kwargs, tracing_options=tracing_options\n    134   )\n\n/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py in trace_function(args, kwargs, tracing_options)\n    176       kwargs = {}\n    177 \n--&gt; 178     concrete_function = _maybe_define_function(\n    179         args, kwargs, tracing_options\n    180     )\n\n/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py in _maybe_define_function(args, kwargs, tracing_options)\n    218     )\n    219 \n--&gt; 220   current_func_context = function_context.make_function_context(\n    221       tracing_options.scope_type\n    222   )\n\n/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/function_context.py in make_function_context(scope_type)\n     90     in_cross_replica_context = (strategy_stack[-1].replica_context is None)  # pylint: disable=protected-access\n     91   except (AttributeError, IndexError):\n---&gt; 92     pass\n     93 \n     94   if save_context.in_save_context():\n\nKeyboardInterrupt: \n\n\n\n\n# Plotting Results\nfor lr in learning_rates:\nplt.plot(history_dict[lr], label=f\"lr={lr}\")\n\nplt.title(\"Validation Loss for Different Learning Rates\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Validation Loss\")\nplt.legend()\nplt.show()\n\nWhen should they pull the bikes out of rotation for cleaning?\n\n# Rentals by Hour\nhourly = df.groupby('hr')['total'].mean()\n\nplt.plot(hourly.index, hourly.values)\nplt.title(\"Average Rentals by Hour\")\nplt.xlabel(\"Hour of Day\")\nplt.ylabel(\"Average Rentals\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Rentals by Day of Week\ndf['day_of_week'] = df['dteday'].dt.dayofweek  # 0 = Monday\n\ndaily = df.groupby('day_of_week')['total'].mean()\n\nplt.bar(daily.index, daily.values)\nplt.title(\"Average Rentals by Day of Week\")\nplt.xlabel(\"Day of Week (0=Mon)\")\nplt.ylabel(\"Average Rentals\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Rentals by Combined Hour and Day\npivot = df.pivot_table(values='total', index='hr', columns='day_of_week', aggfunc='mean')\n\nplt.imshow(pivot, aspect='auto')\nplt.colorbar(label=\"Avg Rentals\")\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Hour\")\nplt.title(\"Heatmap of Rentals\")\nplt.show()\n\n\n\n\n\n\n\n\nWhat can we expect for forcasts into the future? Are we back on track or still recovering?\n\n# Daily Rentals Over Time\ndaily_totals = df.groupby('dteday')['total'].sum()\n\nplt.plot(daily_totals.index, daily_totals.values)\nplt.title(\"Total Rentals Over Time\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Total Rentals\")\nplt.show()\n\n\n\n# Average Trends\nrolling = daily_totals.rolling(window=30).mean()\n\nplt.plot(daily_totals.index, daily_totals.values, alpha=0.3, label=\"Daily\")\nplt.plot(rolling.index, rolling.values, color='red', label=\"30-Day Avg\")\nplt.legend()\nplt.title(\"Trend in Bike Rentals\")\nplt.show()\n\n\n# Comparing Monthly\ndf['month'] = df['dteday'].dt.month\nmonthly = df.groupby('month')['total'].mean()\n\nplt.bar(monthly.index, monthly.values)\nplt.title(\"Average Rentals by Month\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Average Rentals\")\nplt.show()\n\n\n\n# Atempting Future Prediction\nmodel = build_model(0.001)\n\nmodel.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1)\n\npredictions = model.predict(X_val)\n\nplt.scatter(y_val, predictions)\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Model Predictions\")\nplt.show()\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/starter_bank.html",
    "href": "notebooks/starter_bank.html",
    "title": "Jaeli Jaussi - Data Science Portfolio",
    "section": "",
    "text": "# Load Libraries and Data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt\n\nhousing = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')\nhousing.head()\n\n# Clean and Categorize Code\nhousing['zipcode'] = housing['zipcode'].astype(str)\n\nhousing['sqft_per_floor'] = housing['sqft_living'] / housing['floors']\n\nfeatures = [\n    \"condition\",\n    \"grade\",\n    \"view\",\n    \"lat\",\n    \"long\",\n    \"sqft_living\",\n    \"sqft_lot\",\n    \"sqft_lot15\",\n    \"sqft_per_floor\"\n]\n\nX = housing[features]\ny = housing['price']\n\nX = X.join(pd.get_dummies(housing['zipcode'], prefix='zip'))\nX = X.dropna()\ny = y.loc[X.index]\n\n# Train and test Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train Random Forest Model\nrf = RandomForestRegressor(\n    n_estimators=300,\n    random_state=42,\n    max_depth=None,\n    n_jobs=-1\n)\n\nrf.fit(X_train, y_train)\n\n\nRandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=42) \n\n\n\nRMSE = average prediction error in dollars\nR¬≤ = how much price variation your model explains 0.65‚Äì0.80 is very solid for housing data This = quantifiable evidence of reliability\n\n\n# Cecil's Question\n# Evaluate the Model\ny_pred = rf.predict(X_test)\n\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nr2 = r2_score(y_test, y_pred)\n\nrmse, r2\n\n\n# William's Question\n# Feature Importance\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nimportances = importances.sort_values(ascending=False)\n\nimportances.head(10)\n\n# Plotting Feature Importance\nplt.figure(figsize=(10,6))\nimportances.head(10).plot(kind='barh')\nplt.title(\"Top 10 Most Important Features for Predicting Price\")\nplt.gca().invert_yaxis()\nplt.show()\n\n\n# Model Reliability Visualization per Cecil\nplt.figure(figsize=(6,6))\nsns.scatterplot(x=y_test, y=y_pred, alpha=0.3)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted vs Actual Housing Prices\")\nplt.show()\n\nTight diagonal = good predictions\nScatter = error This visually supports reliability claims.\nModel shows: home features, location\nCould potentially join external data: median price per neighborhood, school neighborhood ratings, walkability ratings\nI built a Random Forest regression model using housing features such as square footage, condition, grade, view, and geographic location to predict home prices. I engineered an additional feature representing square footage per floor to capture how living space is distributed across levels. The model was evaluated using RMSE and R¬≤ to provide quantifiable evidence of prediction reliability. Feature importance scores were used to identify which aspects of the property most strongly influence price, addressing stakeholder questions about key drivers of value. Geographic variables were included to support future integration of external neighborhood-level factors.\nAttempting to work with zipcode, lat, long.\n\n# Using \"Homes in area are similiar\"\nzip_stats = housing.groupby('zipcode')['price'].agg([\n    'mean', 'median', 'count'\n]).reset_index()\n\nzip_stats.columns = ['zipcode', 'zip_price_mean', 'zip_price_median', 'zip_count']\n\nhousing = housing.merge(zip_stats, on='zipcode', how='left')\n\nzip_price_mean - average price per area\nzip_count - quantity of homes\n\n# categorize into natural geographical areas\nfrom sklearn.cluster import KMeans\n\ncoords = housing[['lat', 'long']]\n\nkmeans = KMeans(n_clusters=20, random_state=42)\nhousing['geo_cluster'] = kmeans.fit_predict(coords)\n\nX = housing[[\n    \"sqft_living\", \"sqft_lot\", \"condition\", \"grade\", \"view\",\n    \"sqft_lot15\", \"sqft_per_floor\", \"geo_cluster\"\n]]\n\n\n# Creating distance variable\nimport numpy as np\n\n# Approx Seattle city center\nseattle_lat, seattle_long = 47.6062, -122.3321\n\nhousing['dist_to_seattle'] = np.sqrt(\n    (housing['lat'] - seattle_lat)**2 +\n    (housing['long'] - seattle_long)**2\n)\n\n\n# Each zipcode is a redefined category\nhousing_encoded = pd.get_dummies(housing, columns=['zipcode'], prefix='zip')\n\nX = housing_encoded[[\n    \"sqft_living\", \"sqft_lot\", \"condition\", \"grade\", \"view\",\n    \"sqft_lot15\", \"sqft_per_floor\"\n] + [col for col in housing_encoded.columns if col.startswith('zip_')]]\n\n\nhousing['sqft_per_floor'] = housing['sqft_living'] / housing['floors']\n\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=20, random_state=42)\nhousing['geo_cluster'] = kmeans.fit_predict(housing[['lat', 'long']])\n\nzip_stats = housing.groupby('zipcode')['price'].mean().reset_index()\nzip_stats.columns = ['zipcode', 'zip_price_mean']\nhousing = housing.merge(zip_stats, on='zipcode', how='left')\n\nfeatures = [\n    \"sqft_living\",\n    \"sqft_lot\",\n    \"sqft_lot15\",\n    \"condition\",\n    \"grade\",\n    \"view\",\n    \"sqft_per_floor\",\n    \"geo_cluster\",\n    \"zip_price_mean\"\n]\n\nLatitude, longitude, and zipcode do not correlate well with price when treated as raw numerical values. To capture neighborhood-level pricing effects, I transformed location features into meaningful geographic groupings. Homes were clustered into geographic regions using latitude and longitude, creating a neighborhood identifier. Additionally, zipcode-level price averages were computed to capture typical market values by area. These transformations allowed the model to learn area-based pricing patterns and significantly improved predictive performance.\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 1"
    ]
  }
]