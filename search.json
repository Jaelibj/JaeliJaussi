[
  {
    "objectID": "Full_Stack/project5.html",
    "href": "Full_Stack/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 5"
    ]
  },
  {
    "objectID": "Full_Stack/project4.html",
    "href": "Full_Stack/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 4"
    ]
  },
  {
    "objectID": "Full_Stack/project3.html",
    "href": "Full_Stack/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 3"
    ]
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-1-import-pandas-and-load-the-data",
    "href": "notebooks/Exploration_02.html#part-1-import-pandas-and-load-the-data",
    "title": "Data Exploration 02",
    "section": "Part 1: Import Pandas and load the data",
    "text": "Part 1: Import Pandas and load the data\nRemember to import Pandas the conventional way. If youâ€™ve forgotten how, you may want to review Data Exploration 01.\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce youâ€™ve loaded the data, itâ€™s a good idea to poke around a little bit to find out what youâ€™re dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 1: Enter your code below to import Pandas according to the\n# conventional method. Then load the dataset into a Pandas dataframe.\nimport pandas as pd\n\ncereal = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/cereal.csv\")\n\n# Write any code needed to explore the data by seeing what the first few\n# rows look like. Then display a technical summary of the data to determine\n# the data types of each column, and which columns have missing data.\ncereal.head()\n\n\n    \n\n\n\n\n\n\nname\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n0\n100% Bran\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1.0\n0.33\n68.402973\n\n\n1\n100% Natural Bran\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1.0\n1.00\n33.983679\n\n\n2\nAll-Bran\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1.0\n0.33\n59.425505\n\n\n3\nAll-Bran with Extra Fiber\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1.0\n0.50\n93.704912\n\n\n4\nAlmond Delight\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1.0\n0.75\n34.384843"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-2-calculate-summary-statistics",
    "href": "notebooks/Exploration_02.html#part-2-calculate-summary-statistics",
    "title": "Data Exploration 02",
    "section": "Part 2: Calculate Summary Statistics",
    "text": "Part 2: Calculate Summary Statistics\nThe marketing team has determined that when choosing a cereal, consumers are most interested in calories, sugars, fiber, fat, and protein.\nFirst, letâ€™s calcuate some summary statistics for these categories across the entire dataset. Weâ€™re particularly intrested in the mean, median, standard deviation, min, and max values.\nThere are multiple ways to accomplish this.\n\n# Part 2: Enter your code below to calculate summary statistics for the\n# calories, sugars, fiber, fat, and protein features.\n\ncereal[['calories', 'sugars', 'fiber', 'fat', 'protein']].describe()\n\n\n    \n\n\n\n\n\n\ncalories\nsugars\nfiber\nfat\nprotein\n\n\n\n\ncount\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n\n\nmean\n106.883117\n6.922078\n2.151948\n1.012987\n2.545455\n\n\nstd\n19.484119\n4.444885\n2.383364\n1.006473\n1.094790\n\n\nmin\n50.000000\n-1.000000\n0.000000\n0.000000\n1.000000\n\n\n25%\n100.000000\n3.000000\n1.000000\n0.000000\n2.000000\n\n\n50%\n110.000000\n7.000000\n2.000000\n1.000000\n3.000000\n\n\n75%\n110.000000\n11.000000\n3.000000\n2.000000\n3.000000\n\n\nmax\n160.000000\n15.000000\n14.000000\n5.000000\n6.000000"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-3-transform-data",
    "href": "notebooks/Exploration_02.html#part-3-transform-data",
    "title": "Data Exploration 02",
    "section": "Part 3: Transform Data",
    "text": "Part 3: Transform Data\nTo make analysis easier, you want to convert the manufacturer codes used in the dataset to the manufacturer names.\nFirst, display the count of each manufacturer code value used in the dataset (found in the mfr column).\nThen, create a new column with the appropriate manufacturer name for each entry, using this mapping:\nA = American Home Food Products\nG = General Mills\nK = Kelloggs\nN = Nabisco\nP = Post\nQ = Quaker Oats\nR = Ralston Purina\n\nNote: While the tutorial linked above uses the replace function, using the map function instead can often be much faster and more memory efficient, especially for large datasets.\n\n\n# Display the count of values for the manufacturer code (\"mfr\" column), then\n# create a new column containing the appropriate manufacturer names.\ncereal['mfr_name'] = cereal['mfr'].map({'A': 'American Home Food Products',\n    'G': 'General Mills',\n    'K': 'Kelloggs',\n    'N': 'Nabisco',\n    'P': 'Post',\n    'Q': 'Quaker Oats',\n    'R': 'Ralston Purina'})"
  },
  {
    "objectID": "notebooks/Exploration_02.html#part-4-visualization",
    "href": "notebooks/Exploration_02.html#part-4-visualization",
    "title": "Data Exploration 02",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\nLetâ€™s do some more data exploration visually.\nImport your visualization library of choice and set any needed configuration options.\n\n# Import your visualization library\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Select Color\nsns.set(style=\"whitegrid\")\n\n\nSugar Distribution\nMarketing tells us that their surveys have revealed that sugar content is the number one concern of consumers when choosing cereal.\nThey would like to see the following visualizations:\n\nA histogram plot of the sugar content in all cereals.\nA scatter plot showing the relationship between sugar and calories.\nA box plot showing the distribution of sugar content by manufacturer.\n\n\n# Create the three visualzations requested by the the marketing team\n# A histogram plot of the sugar content in all cereals.\nplt.figure(figsize=(8, 5))\nsns.histplot(data=cereal, x='sugars', bins=10)\nplt.title(\"Distribution of Sugar Content in Cereals\")\nplt.xlabel(\"Sugar (grams)\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# A scatter plot showing the relationship between sugar and calories.\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=cereal, x='sugars', y='calories')\nplt.title(\"Sugar vs Calories in Cereals\")\nplt.xlabel(\"Sugar (grams)\")\nplt.ylabel(\"Calories\")\nplt.show()\n\n# A box plot showing the distribution of sugar content by manufacturer.\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=cereal, x='mfr_name', y='sugars')\nplt.title(\"Sugar Content by Manufacturer\")\nplt.xlabel(\"Manufacturer\")\nplt.ylabel(\"Sugar (grams)\")\nplt.xticks(rotation=45)\nplt.show()"
  },
  {
    "objectID": "notebooks/Exploration_02.html#above-and-beyond",
    "href": "notebooks/Exploration_02.html#above-and-beyond",
    "title": "Data Exploration 02",
    "section": "ğŸŒŸ Above and Beyond ğŸŒŸ",
    "text": "ğŸŒŸ Above and Beyond ğŸŒŸ\nThe marketing team is pleased with what youâ€™ve accomplished so far. They have a meeting with top cereal executives in the morning, and theyâ€™d like you to do as many of the following additional tasks as you have time for:\n\nWeight Watchers used to have an older points system that used this formula: (calories / 50) + (fat / 12) - (fiber / 5), but only the first 4 grams of fiber were included in the calculation. For comparisonâ€™s sake, create an additional column with the calculation for the old points system.\nMarketing really likes the boxplot of the sugar content for each cereal, theyâ€™d like similar plots for calories and fat, but using different color schemes for each chart."
  },
  {
    "objectID": "notebooks/Exploration_01.html#mpaa-movie-ratings",
    "href": "notebooks/Exploration_01.html#mpaa-movie-ratings",
    "title": "Data Exploration 01",
    "section": "MPAA Movie Ratings:",
    "text": "MPAA Movie Ratings:\n\nG: All ages admitted.\nPG: Some material may not be suitable for children.\nPG-13: Some material may be inappropriate for children under 13.\nR: Under 17 requires accompanying parent or adult guardian\nNC-17: No One 17 and Under Admitted\n\nMost people would consider G and PG as ratings suitable for children. However, not everyone would agree that a PG-13 movie is necssarily a childrenâ€™s movie. It is up to you to decide how to handle this."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-1-import-pandas",
    "href": "notebooks/Exploration_01.html#part-1-import-pandas",
    "title": "Data Exploration 01",
    "section": "Part 1: Import Pandas",
    "text": "Part 1: Import Pandas\nThe pandas library is a python library used for data analysis and manipulation. It will provide the core functionality for most of what you do in the data exploration and preprocessing stages of most machine learning projects.\nPlease see this Getting Started Guide for information on the conventional way to import Pandas into your project, as well as other helpful tips for common Pandas tasks.\n\n# Part 1: Enter the code below to import Pandas according to the\n# conventional method."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-2-load-the-data",
    "href": "notebooks/Exploration_01.html#part-2-load-the-data",
    "title": "Data Exploration 01",
    "section": "Part 2: Load the data",
    "text": "Part 2: Load the data\nThe dataset for this exploration is stored at the following url:\nhttps://raw.githubusercontent.com/byui-cse/cse450-course/master/data/netflix_titles.csv\nThere are lots of ways to load data into your workspace. The easiest way in this case is to ask Pandas to do it for you.\n\nInitial Data Analysis\nOnce youâ€™ve loaded the data, itâ€™s a good idea to poke around a little bit to find out what youâ€™re dealing with.\nSome questions you might ask include:\n\nWhat does the data look like?\nWhat kind of data is in each column?\nDo any of the columns have missing values?\n\n\n# Part 2: Load the dataset into a Pandas dataframe.\n\n\n# Then, explore the data by seeing what the first few rows look like.\n\n\n# Next, display a technical summary of the data to determine the data types of each column, and which columns have missing data."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-3-filter-the-data",
    "href": "notebooks/Exploration_01.html#part-3-filter-the-data",
    "title": "Data Exploration 01",
    "section": "Part 3: Filter the Data",
    "text": "Part 3: Filter the Data\nSince weâ€™re just interested in movies, weâ€™ll need to filter out anything that isnâ€™t a movie for our analysis. The type feature contains this information.\nOnce we have the subset, we should see how many rows it contains. There are a variety of ways to get the length of a data frame.\n\n# Use pandas's filtering abilitites to select the subset of data\n# that represents movies, then calculate how many rows are in the filtered data.\n\n\nMPAA Ratings\nNow that we have only movies, letâ€™s get a quick count of the values being used in the rating feature.\n\n# Determine the number of records for each value of the \"rating\" feature.\n# Remember to count the values in your subset only, not in the original dataframe.\n\n\n\nMore Filtering\nThere are apparently some â€œmade for TVâ€ movies in the list that donâ€™t fit the MPAA rating scheme.\nLetâ€™s filter some more to just see movies rated with the standard MPAA ratings of G, PG, PG-13, R, and NC-17.\n\n# Filter the list of movies to select a new subset containing only movies with\n# a standard MPAA rating. Calculate how many rows are in this new set, and\n# then see which ratings appear most often."
  },
  {
    "objectID": "notebooks/Exploration_01.html#part-4-visualization",
    "href": "notebooks/Exploration_01.html#part-4-visualization",
    "title": "Data Exploration 01",
    "section": "Part 4: Visualization",
    "text": "Part 4: Visualization\nNow that we have explored and preprocessed our data, letâ€™s create a visualization to summarize our findings.\n\nExploration vs Presentation\nBroadly speaking, there are two types of visualizations: * Barebones visualizations you might use to get a quick, visual understanding of the data while youâ€™re trying to decide how it all fits together. * Presentation-quality visualizations that you would include in a report or presentation for management or other stakeholders.\n\n\nVisualization Tools\nThere are many different visualization tools availble. In the sections below, weâ€™ll explore the three most common. Each of these libraries has strengths and weaknesses.\nIt is probably a good idea for you to become familiar with each one, and then become proficient at whichever one you like the best.\n\n\nAltair\nThe Altair visualization library provides a large variety of very easy to use statistical charting tools.\nAltair uses a declarative language to build up charts piece by piece.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nalt.Chart(employees).mark_boxplot().encode(\n    x='job',\n    y='salary'\n)\n\n# Make a box plot style categorical plot, and customize the results\nalt.Chart(employees).mark_boxplot().encode(\n    alt.X('job', title='Job title'),\n    alt.Y('salary', title='Annual salary in thousands of $USD')\n).properties(\n  title='Salaries by Job Title'\n)\nLike with Pandas, there is a conventional way to import Altair into your projects.\n\n# Import the Altair library the conventional way.\n\nLetâ€™s create a barchart showing the count of each movie rating by using Altairâ€™s aggregation capabilities.\nIn this example, we see the x axis being set to a feature called a, and the y axis set to the average() of a feature called b.\nIn our case, we want the x axis to be set to rating and the y axis to be the count() of rating.\n\n# Use Altair to create a bar chart comparing the count of each movie rating\n\n\n\nSeaborn\nWhile Altair uses a â€œdeclarativeâ€ syntax for building charts piece by piece, the Seaborn library provides a large variety of pre-made charts for common statistical needs.\nThese charts are divided into different categories. Each category has a high-level interface you can use for simplicity, and then a specific function for each chart that you can use if you need more control over how the chart looks.\nSeaborn uses matplotlib for its drawing, and the chart-specific functions each return a matplitlib axes object if you need additional customization.\nFor example, there are several different types of categorical plots in seaborn: bar plots, box plots, point plots, count plots, swarm plots, etcâ€¦\nEach of these plots can be accessed using the catplot function.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nsns.catplot(data=employees, x='job', y='salary', kind='box')\n\n# Make a swarm plot style categorical plot\nsns.catplot(data=employees, x='job', y='salary', kind='swarm')\nAlternatively, you can use the plot specific functions to give yourself more control over the output by using matplotlib functions:\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = sns.boxplot(data=employees, x='job', y='salary')\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\nLike with Pandas, there is a conventional way to import Seaborn into your projects.\nOptionally, you may wish to set some default chart aesthetics by setting the chart style.\n\n# Import the seaborn library the conventional way. Then optionally configure\n# the default chart style.\n\nSince the rating column uses categorical data, we need to use Seabornâ€™s categorical visualizations.\nIn particular, we want a â€œcount plotâ€ that will display a count of movie ratings.\n\n# Use seaborn to create a count plot comparing the count of each movie rating\n\n\n\nPandas built-in plotting\nIn addition to libraries like Altair and Seaborn, Pandas has some built in charting functionality.\nWhile not as sophisticated as some of the other options, it is often good enough for quick visualizations.\nJust like with seabornâ€™s plotting functions, the pandas plotting functions return matplotlib axes objects, which can be further customized.\nAssume we have a pandas dataframe called employees, with three columns: name, job, salary.\n# Make a box plot style categorical plot showing the distribution of salaries for each job:\nemployees[ ['job','salary'] ].plot.box()\n\n# Make a box plot style categorical plot, and customize the results\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nax = employees[ ['job','salary'] ].plot().box()\nax.set_title(\"Salaries by Job Title\")\nax.set_ylabel(\"Annual salary in thousands of $USD\")\nax.set_xlabel(\"Job title\")\n\n# Use pandas' built in plotting functions to create a count plot comparing the count of each movie rating\n# This will be a little trickier than the other libraries, but one hint is that the pandas value_counts() function\n# actually returns a dataframe."
  },
  {
    "objectID": "notebooks/Exploration_01.html#above-and-beyond",
    "href": "notebooks/Exploration_01.html#above-and-beyond",
    "title": "Data Exploration 01",
    "section": "ğŸŒŸ Above and Beyond ğŸŒŸ",
    "text": "ğŸŒŸ Above and Beyond ğŸŒŸ\nAfter reviewing your findings, the watchdog group would like some additional questions answered:\n\nHow are things affected if you include the â€œmade for TV moviesâ€ that have been assigned TV ratings in your analysis, but still exclude unrated movies?\nThey would also like to see a separate report that includes only TV shows.\nFor an upcoming community meeting, the group would like to present a simple chart showing â€œFor Kidsâ€ and â€œFor Adultsâ€ categories. The easiest way to accomplish this would be to create a new column in your data frame that maps each rating to the appropriate â€œFor Kidsâ€ or â€œFor Adultsâ€ label, then create a new visualization based on that column."
  },
  {
    "objectID": "Cleansing_Exploration/project1.html",
    "href": "Cleansing_Exploration/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project2.html",
    "href": "Cleansing_Exploration/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Full Stack"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project5.html",
    "href": "Cleansing_Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project4.html",
    "href": "Cleansing_Projects/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 4"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project3.html",
    "href": "Cleansing_Projects/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 3"
    ]
  },
  {
    "objectID": "Story_Telling/project1.html",
    "href": "Story_Telling/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 1"
    ]
  },
  {
    "objectID": "Story_Telling/project2.html",
    "href": "Story_Telling/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 2"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Isaac Newtonsâ€™s CV",
    "section": "",
    "text": "Physicist, Mathematician, Cambridge professor.\n\nisaac@applesdofall.org | My wikipedia page\n\n\n\nStanding on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples.\n\n\n\n\n1654-1660 The Kingâ€™s School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow\n\n\n\n\n2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France\n\n\n\n\n\n\n1669 Newton Sir I, De analysi per Ã¦quationes numero terminorum infinitas.\n1669 Lectiones opticÃ¦.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001\n\n\n\n\n1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Isaac Newtonsâ€™s CV",
    "section": "",
    "text": "Standing on the shoulders of giants\n\n\nLaws of motion, gravitation, minting coins, disliking Robert Hooke\n\n\n\nCooling, power series, optics, alchemy, planetary motions, apples."
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Isaac Newtonsâ€™s CV",
    "section": "",
    "text": "1654-1660 The Kingâ€™s School, Grantham.\nJune 1661 - now Trinity College, Cambridge\n\nSizar\n\n1667 - death Trinity College, Cambridge\n\nFellow"
  },
  {
    "objectID": "resume.html#awards",
    "href": "resume.html#awards",
    "title": "Isaac Newtonsâ€™s CV",
    "section": "",
    "text": "2012 President, Royal Society, London, UK\nAssociate, French Academy of Science, Paris, France"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "Isaac Newtonsâ€™s CV",
    "section": "",
    "text": "1669 Newton Sir I, De analysi per Ã¦quationes numero terminorum infinitas.\n1669 Lectiones opticÃ¦.\netc. etc. etc.\n\n\n\n2012 Infinitesimal calculus for solutions to physics problems, SMBC patent 001"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Isaac Newtonsâ€™s CV",
    "section": "",
    "text": "1600 Royal Mint, London\n\nWarden\nMinted coins\n\n1600 Lucasian professor of Mathematics, Cambridge University"
  },
  {
    "objectID": "Machine_Learning/project5.html",
    "href": "Machine_Learning/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 5"
    ]
  },
  {
    "objectID": "Machine_Learning/project4.html",
    "href": "Machine_Learning/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 4"
    ]
  },
  {
    "objectID": "Machine_Learning/project3.html",
    "href": "Machine_Learning/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 3"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Exploration"
    ]
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Competition"
    ]
  },
  {
    "objectID": "Competition/project1.html",
    "href": "Competition/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 1"
    ]
  },
  {
    "objectID": "Competition/project2.html",
    "href": "Competition/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 2"
    ]
  },
  {
    "objectID": "Competition/project3.html",
    "href": "Competition/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 3"
    ]
  },
  {
    "objectID": "Competition/project4.html",
    "href": "Competition/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 4"
    ]
  },
  {
    "objectID": "Competition/project5.html",
    "href": "Competition/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Competition",
      "Project 5"
    ]
  },
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Story Telling"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "Machine_Learning/project2.html",
    "href": "Machine_Learning/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 2"
    ]
  },
  {
    "objectID": "Machine_Learning/project1.html",
    "href": "Machine_Learning/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Machine Learning",
      "Project 1"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Story_Telling/project3.html",
    "href": "Story_Telling/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 3"
    ]
  },
  {
    "objectID": "Story_Telling/project4.html",
    "href": "Story_Telling/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 4"
    ]
  },
  {
    "objectID": "Story_Telling/project5.html",
    "href": "Story_Telling/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Story Telling",
      "Project 5"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project2.html",
    "href": "Cleansing_Projects/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 2"
    ]
  },
  {
    "objectID": "Cleansing_Projects/project1.html",
    "href": "Cleansing_Projects/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Data Cleansing",
      "Project 1"
    ]
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics",
    "crumbs": [
      "Data Cleansing"
    ]
  },
  {
    "objectID": "Cleansing_Exploration/project3.html",
    "href": "Cleansing_Exploration/project3.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project4.html",
    "href": "Cleansing_Exploration/project4.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Cleansing_Exploration/project5.html",
    "href": "Cleansing_Exploration/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebooks/starter_bikes.html",
    "href": "notebooks/starter_bikes.html",
    "title": "Step-by-Step Approach",
    "section": "",
    "text": "import pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nbikes = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\nimport pandas as pd import numpy as np\nMake numpy values easier to read. np.set_printoptions(precision=3, suppress=True)\nimport tensorflow as tf from tensorflow.keras import layers\nimport numpy as np import pandas as pd import tensorflow as tf\nSHUFFLE_BUFFER = 500 BATCH_SIZE = 2\nnumeric_feature_names = [â€˜ageâ€™, â€˜thalachâ€™, â€˜trestbpsâ€™, â€˜cholâ€™, â€˜oldpeakâ€™] numeric_features = df[numeric_feature_names] numeric_features.head()\nnormalizer = tf.keras.layers.Normalization(axis=-1) normalizer.adapt(np.array(numeric_features))\nnormalizer(numeric_features.iloc[:3])\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nWhat approach will we use to find the optimal learning rate?"
  },
  {
    "objectID": "notebooks/starter_bikes.html#major-trial-and-error-technique",
    "href": "notebooks/starter_bikes.html#major-trial-and-error-technique",
    "title": "Step-by-Step Approach",
    "section": "Major Trial and Error Technique",
    "text": "Major Trial and Error Technique\n\n# Load data\ndf = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n\n\n# Convert date\ndf['dteday'] = pd.to_datetime(df['dteday'])\n\n# Create total rentals (target variable)\ndf['total'] = df['casual'] + df['registered']\n\n# Drop columns we won't use\ndf = df.drop(columns=['casual', 'registered'])\n\n# One-hot encode categorical variables\ndf = pd.get_dummies(df, columns=['season', 'weathersit'], drop_first=True)\n\n# Features and target\nX = df.drop(columns=['dteday', 'total'])\ny = df['total']\n\n# Scale features\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train/test split\nX_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n\n# Learning Model Trial\ndef build_model(lr):\n    model = Sequential([\n        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n        Dense(32, activation='relu'),\n        Dense(1)\n    ])\n\n    model.compile(\n        optimizer=Adam(learning_rate=lr),\n        loss='mse',\n        metrics=['mae']\n    )\n\n    return model\n\nlearning_rates = [0.1, 0.01, 0.001, 0.0001]\nhistory_dict = {}\n\nfor lr in learning_rates:\n    print(f\"\\nTraining with learning rate: {lr}\")\n\n    model = build_model(lr)\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val),\n        epochs=20,\n        batch_size=32,\n        verbose=0\n    )\n\n    history_dict[lr] = history.history['val_loss']\n\n\n\nTraining with learning rate: 0.1\n\nTraining with learning rate: 0.01\n\nTraining with learning rate: 0.001\n\nTraining with learning rate: 0.0001\n\n\n\n# Plotting Results\nfor lr in learning_rates:\nplt.plot(history_dict[lr], label=f\"lr={lr}\")\n\nplt.title(\"Validation Loss for Different Learning Rates\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Validation Loss\")\nplt.legend()\nplt.show()\n\n\n  File \"/tmp/ipython-input-4239491910.py\", line 3\n    plt.plot(history_dict[lr], label=f\"lr={lr}\")\n    ^\nIndentationError: expected an indented block after 'for' statement on line 2\n\n\n\n\nWhen should they pull the bikes out of rotation for cleaning?\n\n# Rentals by Hour\nhourly = df.groupby('hr')['total'].mean()\n\nplt.plot(hourly.index, hourly.values)\nplt.title(\"Average Rentals by Hour\")\nplt.xlabel(\"Hour of Day\")\nplt.ylabel(\"Average Rentals\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Rentals by Day of Week\ndf['day_of_week'] = df['dteday'].dt.dayofweek  # 0 = Monday\n\ndaily = df.groupby('day_of_week')['total'].mean()\n\nplt.bar(daily.index, daily.values)\nplt.title(\"Average Rentals by Day of Week\")\nplt.xlabel(\"Day of Week (0=Mon)\")\nplt.ylabel(\"Average Rentals\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Rentals by Combined Hour and Day\npivot = df.pivot_table(values='total', index='hr', columns='day_of_week', aggfunc='mean')\n\nplt.imshow(pivot, aspect='auto')\nplt.colorbar(label=\"Avg Rentals\")\nplt.xlabel(\"Day of Week\")\nplt.ylabel(\"Hour\")\nplt.title(\"Heatmap of Rentals\")\nplt.show()\n\n\n\n\n\n\n\n\nWhat can we expect for forcasts into the future? Are we back on track or still recovering?\n\n# Daily Rentals Over Time\ndaily_totals = df.groupby('dteday')['total'].sum()\n\nplt.plot(daily_totals.index, daily_totals.values)\nplt.title(\"Total Rentals Over Time\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Total Rentals\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Average Trends\nrolling = daily_totals.rolling(window=30).mean()\n\nplt.plot(daily_totals.index, daily_totals.values, alpha=0.3, label=\"Daily\")\nplt.plot(rolling.index, rolling.values, color='red', label=\"30-Day Avg\")\nplt.legend()\nplt.title(\"Trend in Bike Rentals\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Comparing Monthly\ndf['month'] = df['dteday'].dt.month\nmonthly = df.groupby('month')['total'].mean()\n\nplt.bar(monthly.index, monthly.values)\nplt.title(\"Average Rentals by Month\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Average Rentals\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Atempting Future Prediction\nmodel = build_model(0.001)\n\nmodel.fit(X_train, y_train, epochs=30, batch_size=32, verbose=1)\n\npredictions = model.predict(X_val)\n\nplt.scatter(y_val, predictions)\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title(\"Model Predictions\")\nplt.show()\n\nEpoch 1/30\n\n\n/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 126761.1328 - mae: 256.1093\n\nEpoch 2/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 75683.5312 - mae: 200.5388\n\nEpoch 3/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 68363.9453 - mae: 187.3937\n\nEpoch 4/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 62739.8320 - mae: 177.2837\n\nEpoch 5/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 59169.7383 - mae: 172.6404\n\nEpoch 6/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 58070.2578 - mae: 172.1947\n\nEpoch 7/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 55758.3633 - mae: 167.9120\n\nEpoch 8/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 54478.8281 - mae: 165.5332\n\nEpoch 9/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 10s 2ms/step - loss: 53827.7617 - mae: 164.3397\n\nEpoch 10/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 52059.3750 - mae: 160.6890\n\nEpoch 11/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 51613.4180 - mae: 159.2699\n\nEpoch 12/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 49746.4805 - mae: 155.7126\n\nEpoch 13/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 49534.0000 - mae: 154.3789\n\nEpoch 14/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 48629.0820 - mae: 152.0804\n\nEpoch 15/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 47068.6211 - mae: 148.2944\n\nEpoch 16/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 46263.7852 - mae: 145.5696\n\nEpoch 17/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 45717.3516 - mae: 142.7186\n\nEpoch 18/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 44579.1172 - mae: 140.0195\n\nEpoch 19/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 43617.9453 - mae: 137.9724\n\nEpoch 20/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 9s 2ms/step - loss: 42625.4805 - mae: 135.0622\n\nEpoch 21/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 41845.3984 - mae: 132.9548\n\nEpoch 22/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 41553.8359 - mae: 132.1180\n\nEpoch 23/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 41427.5117 - mae: 131.3552\n\nEpoch 24/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 39540.7109 - mae: 128.5079\n\nEpoch 25/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 39911.1992 - mae: 128.6778\n\nEpoch 26/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 39219.5391 - mae: 128.0282\n\nEpoch 27/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 38160.2383 - mae: 126.3341\n\nEpoch 28/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 37744.0742 - mae: 125.6271\n\nEpoch 29/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 5s 2ms/step - loss: 36712.4336 - mae: 124.4800\n\nEpoch 30/30\n\n2812/2812 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 6s 2ms/step - loss: 36453.2734 - mae: 123.9342\n\n703/703 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 1ms/step"
  },
  {
    "objectID": "notebooks/starter_bank.html",
    "href": "notebooks/starter_bank.html",
    "title": "Jaeli Jaussi - Data Science Portfolio",
    "section": "",
    "text": "# Load Libraries and Data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt\n\nhousing = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')\nhousing.head()\n\n# Clean and Categorize Code\nhousing['zipcode'] = housing['zipcode'].astype(str)\n\nhousing['sqft_per_floor'] = housing['sqft_living'] / housing['floors']\n\nfeatures = [\n    \"condition\",\n    \"grade\",\n    \"view\",\n    \"lat\",\n    \"long\",\n    \"sqft_living\",\n    \"sqft_lot\",\n    \"sqft_lot15\",\n    \"sqft_per_floor\"\n]\n\nX = housing[features]\ny = housing['price']\n\nX = X.join(pd.get_dummies(housing['zipcode'], prefix='zip'))\nX = X.dropna()\ny = y.loc[X.index]\n\n# Train and test Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Train Random Forest Model\nrf = RandomForestRegressor(\n    n_estimators=300,\n    random_state=42,\n    max_depth=None,\n    n_jobs=-1\n)\n\nrf.fit(X_train, y_train)\n\n\nRandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(n_estimators=300, n_jobs=-1, random_state=42) \n\n\n\nRMSE = average prediction error in dollars\nRÂ² = how much price variation your model explains 0.65â€“0.80 is very solid for housing data This = quantifiable evidence of reliability\n\n\n# Cecil's Question\n# Evaluate the Model\ny_pred = rf.predict(X_test)\n\nrmse = sqrt(mean_squared_error(y_test, y_pred))\nr2 = r2_score(y_test, y_pred)\n\nrmse, r2\n\n\n# William's Question\n# Feature Importance\nimportances = pd.Series(rf.feature_importances_, index=X.columns)\nimportances = importances.sort_values(ascending=False)\n\nimportances.head(10)\n\n# Plotting Feature Importance\nplt.figure(figsize=(10,6))\nimportances.head(10).plot(kind='barh')\nplt.title(\"Top 10 Most Important Features for Predicting Price\")\nplt.gca().invert_yaxis()\nplt.show()\n\n\n# Model Reliability Visualization per Cecil\nplt.figure(figsize=(6,6))\nsns.scatterplot(x=y_test, y=y_pred, alpha=0.3)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted vs Actual Housing Prices\")\nplt.show()\n\nTight diagonal = good predictions\nScatter = error This visually supports reliability claims.\nModel shows: home features, location\nCould potentially join external data: median price per neighborhood, school neighborhood ratings, walkability ratings\nI built a Random Forest regression model using housing features such as square footage, condition, grade, view, and geographic location to predict home prices. I engineered an additional feature representing square footage per floor to capture how living space is distributed across levels. The model was evaluated using RMSE and RÂ² to provide quantifiable evidence of prediction reliability. Feature importance scores were used to identify which aspects of the property most strongly influence price, addressing stakeholder questions about key drivers of value. Geographic variables were included to support future integration of external neighborhood-level factors.\nAttempting to work with zipcode, lat, long.\n\n# Using \"Homes in area are similiar\"\nzip_stats = housing.groupby('zipcode')['price'].agg([\n    'mean', 'median', 'count'\n]).reset_index()\n\nzip_stats.columns = ['zipcode', 'zip_price_mean', 'zip_price_median', 'zip_count']\n\nhousing = housing.merge(zip_stats, on='zipcode', how='left')\n\nzip_price_mean - average price per area\nzip_count - quantity of homes\n\n# categorize into natural geographical areas\nfrom sklearn.cluster import KMeans\n\ncoords = housing[['lat', 'long']]\n\nkmeans = KMeans(n_clusters=20, random_state=42)\nhousing['geo_cluster'] = kmeans.fit_predict(coords)\n\nX = housing[[\n    \"sqft_living\", \"sqft_lot\", \"condition\", \"grade\", \"view\",\n    \"sqft_lot15\", \"sqft_per_floor\", \"geo_cluster\"\n]]\n\n\n# Creating distance variable\nimport numpy as np\n\n# Approx Seattle city center\nseattle_lat, seattle_long = 47.6062, -122.3321\n\nhousing['dist_to_seattle'] = np.sqrt(\n    (housing['lat'] - seattle_lat)**2 +\n    (housing['long'] - seattle_long)**2\n)\n\n\n# Each zipcode is a redefined category\nhousing_encoded = pd.get_dummies(housing, columns=['zipcode'], prefix='zip')\n\nX = housing_encoded[[\n    \"sqft_living\", \"sqft_lot\", \"condition\", \"grade\", \"view\",\n    \"sqft_lot15\", \"sqft_per_floor\"\n] + [col for col in housing_encoded.columns if col.startswith('zip_')]]\n\n\nhousing['sqft_per_floor'] = housing['sqft_living'] / housing['floors']\n\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=20, random_state=42)\nhousing['geo_cluster'] = kmeans.fit_predict(housing[['lat', 'long']])\n\nzip_stats = housing.groupby('zipcode')['price'].mean().reset_index()\nzip_stats.columns = ['zipcode', 'zip_price_mean']\nhousing = housing.merge(zip_stats, on='zipcode', how='left')\n\nfeatures = [\n    \"sqft_living\",\n    \"sqft_lot\",\n    \"sqft_lot15\",\n    \"condition\",\n    \"grade\",\n    \"view\",\n    \"sqft_per_floor\",\n    \"geo_cluster\",\n    \"zip_price_mean\"\n]\n\nLatitude, longitude, and zipcode do not correlate well with price when treated as raw numerical values. To capture neighborhood-level pricing effects, I transformed location features into meaningful geographic groupings. Homes were clustered into geographic regions using latitude and longitude, creating a neighborhood identifier. Additionally, zipcode-level price averages were computed to capture typical market values by area. These transformations allowed the model to learn area-based pricing patterns and significantly improved predictive performance.\n\n\n\n Back to top"
  },
  {
    "objectID": "Full_Stack/project2.html",
    "href": "Full_Stack/project2.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 2"
    ]
  },
  {
    "objectID": "Full_Stack/project1.html",
    "href": "Full_Stack/project1.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "Paste in a template\n\n\n\n\n Back to top",
    "crumbs": [
      "Full Stack",
      "Project 1"
    ]
  }
]